[{"authors":["admin"],"categories":null,"content":"Weiyuan is a PhD student at SFU right now. His research interests include Data Management, ML Debugging and applying Machine Learning to real problems.\nHe took a gap year working in Strikingly @ Shanghai, as a Data Scientist.\nHe can be found at SFU Database System Lab.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1643765003,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wooya.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Weiyuan is a PhD student at SFU right now. His research interests include Data Management, ML Debugging and applying Machine Learning to real problems.\nHe took a gap year working in Strikingly @ Shanghai, as a Data Scientist.","tags":null,"title":"","type":"authors"},{"authors":["Jinglin Peng","Weiyuan Wu","Brandon Lockhart","Song Bian","Jing Nathan Yan","Linghao Xu","Zhixuan Chi","Jeffrey Rzeszotarski","Jiannan Wang"],"categories":null,"content":"","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"de5483a71a8f8349ade04aae9e2e9f56","permalink":"https://wooya.me/publication/dataprep.eda/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/publication/dataprep.eda/","section":"publication","summary":"Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: this https URL.\n","tags":["DataPrep","Exploratory Data Analysis"],"title":"DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python","type":"publication"},{"authors":["Yejia Liu","","Lampros Flokas","Jiannan Wang","Eugene Wu"],"categories":null,"content":"","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"637c8bb086acc3d8e72c864f3939b733","permalink":"https://wooya.me/publication/flrain/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/publication/flrain/","section":"publication","summary":"How can we debug a logistical regression model in a federated learning setting when seeing the model behave unexpectedly (e.g., the model rejects all high-income customers' loan applications)? The SQL-based training data debugging framework has proved effective to fix this kind of issue in a non-federated learning setting. Given an unexpected query result over model predictions, this framework automatically removes the label errors from training data such that the unexpected behavior disappears in the retrained model. In this paper, we enable this powerful framework for federated learning. The key challenge is how to develop a security protocol for federated debugging which is proved to be secure, efficient, and accurate. Achieving this goal requires us to investigate how to seamlessly integrate the techniques from multiple fields (Databases, Machine Learning, and Cybersecurity). We first propose FedRain, which extends Rain, the state-of-the-art SQL-based training data debugging framework, to our federated learning setting. We address several technical challenges to make FedRain work and analyze its security guarantee and time complexity. The analysis results show that FedRain falls short in terms of both efficiency and security. To overcome these limitations, we redesign our security protocol and propose Frog, a novel SQL-based training data debugging framework tailored for federated learning. Our theoretical analysis shows that Frog is more secure, more accurate, and more efficient than FedRain. We conduct extensive experiments using several real-world datasets and a case study. The experimental results are consistent with our theoretical analysis and validate the effectiveness of Frog in practice.\n","tags":["Data Cleaning","Data Debugging"],"title":"Enabling SQL-based Training Data Debugging for Federated Learning","type":"publication"},{"authors":["Xiaoying Wang","Changbo Qu","","Jiannan Wang","Qingqing Zhou"],"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"26100ba8e8bac6489f2a55cbb32b0c12","permalink":"https://wooya.me/publication/vldb-2021-ce/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/publication/vldb-2021-ce/","section":"publication","summary":"Cardinality estimation is a fundamental but long unresolved problem in query optimization. Recently, multiple papers from different research groups consistently report that learned models have the potential to replace existing cardinality estimators. In this paper, we ask a forward-thinking question: Are we ready to deploy these learned cardinality models in production? Our study consists of three main parts. Firstly, we focus on the static environment (i.e., no data updates) and compare five new learned methods with eight traditional methods on four real-world datasets under a unified workload setting. The results show that learned models are indeed more accurate than traditional methods, but they often suffer from high training and inference costs. Secondly, we explore whether these learned models are ready for dynamic environments (i.e., frequent data updates). We find that they cannot catch up with fast data up-dates and return large errors for different reasons. For less frequent updates, they can perform better but there is no clear winner among themselves. Thirdly, we take a deeper look into learned models and explore when they may go wrong. Our results show that the performance of learned methods can be greatly affected by the changes in correlation, skewness, or domain size. More importantly, their behaviors are much harder to interpret and often unpredictable. Based on these findings, we identify two promising research directions (control the cost of learned models and make learned models trustworthy) and suggest a number of research opportunities. We hope that our study can guide researchers and practitioners to work together to eventually push learned cardinality estimators into real database systems.\n","tags":["Cardinality Estimation"],"title":"Are We Ready For Learned Cardinality Estimation?","type":"publication"},{"authors":["Brandon Lockhart","Jinglin Peng","Weiyuan Wu","Jiannan Wang","Eugene Wu"],"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"6e04e0159532764b6e8d7a1b384f8854","permalink":"https://wooya.me/publication/boexplain/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/publication/boexplain/","section":"publication","summary":"Obtaining an explanation for an SQL query result can enrich the analysis experience, reveal data errors, and provide deeper insight into the data. Inference query explanation seeks to explain unexpected aggregate query results on inference data; such queries are challenging to explain because an explanation may need to be derived from the source, training, or inference data in an ML pipeline. In this paper, we model an objective function as a black-box function and propose BOExplain, a novel framework for explaining inference queries using Bayesian optimization (BO). An explanation is a predicate defining the input tuples that should be removed so that the query result of interest is significantly affected. BO - a technique for finding the global optimum of a black-box function - is used to find the best predicate. We develop two new techniques (individual contribution encoding and warm start) to handle categorical variables. We perform experiments showing that the predicates found by BOExplain have a higher degree of explanation compared to those found by the state-of-the-art query explanation engines. We also show that BOExplain is effective at deriving explanations for inference queries from source and training data on a variety of real-world datasets. BOExplain is open-sourced as a Python package at this https URL.\n","tags":["Data Cleaning","Data Debugging"],"title":"Explaining Inference Queries with Bayesian Optimization","type":"publication"},{"authors":null,"categories":["talk"],"content":"","date":1605196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"0be536ca9484fdcc990eff5773394291","permalink":"https://wooya.me/talk/connector-pydata-global-2020/","publishdate":"2020-11-13T00:00:00+08:00","relpermalink":"/talk/connector-pydata-global-2020/","section":"talk","summary":"Data scientists often need to get data from different websites (e.g. Yelp, Twitter, Spotify) via their Web APIs.  In this talk, we will present DataPrep.connector, a unified API wrapper in Python. It enables data scientists to get data from different websites using the same programming interface,  significantly simplifying web data collection. This talk will be of interest to all data scientists.\"\n","tags":["dataprep","pydata"],"title":"A Unified API Wrapper To Simplify Web Data Collection","type":"talk"},{"authors":null,"categories":["talk"],"content":"","date":1592323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"09278b6d87d6dba1b48c7342ba21b361","permalink":"https://wooya.me/talk/rain-sigmod-2020/","publishdate":"2020-06-17T00:00:00+08:00","relpermalink":"/talk/rain-sigmod-2020/","section":"talk","summary":"As the need for machine learning (ML) increases rapidly across all industry sectors,  there is a significant interest among commercial database providers to support \"Query 2.0\",  which integrates model inference into SQL queries. Debugging Query 2.0 is very challenging since an unexpected query result may be caused by the bugs in training data (e.g., wrong labels, corrupted features). In response, we propose Rain, a complaint-driven training data debugging system.  Rain allows users to specify complaints over the query's intermediate or final output,  and aims to return a minimum set of training examples so that if they were removed, the complaints would be resolved. To the best of our knowledge, we are the first to study this problem.  A naive solution requires retraining an exponential number of ML models.  We propose two novel heuristic approaches based on influence functions which both require linear retraining steps.  We provide an in-depth analytical and empirical analysis of the two approaches and conduct extensive experiments  to evaluate their effectiveness using four real-world datasets.  Results show that Rain achieves the highest recall@k among all the baselines while still returns results interactively.\n","tags":["rain","machine learning debugging","SQL explanation"],"title":"Complaint-driven Training Data Debugging","type":"talk"},{"authors":["","Lampros Flokas","Eugene Wu","Jiannan Wang"],"categories":null,"content":"This is a demo paper.\n","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"4d1ba9baf3c32f7de4d2a4aa4199cfc7","permalink":"https://wooya.me/publication/mlops-2020-rain/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/publication/mlops-2020-rain/","section":"publication","summary":"As the need for ML increases rapidly across all industry sectors, there is a significant interest in integrating model inference into critical decision making workflows.  Debugging ML enabled workflows is very challenging since an unexpected workflow result may be caused by errors in training data (e.g. wrong labels or corrupted features).  In response, we envision a complaint-driven data debugging system that allow users to specify complaints over the workflow's output.  As a stepping stone towards such a general system, we build Rain, a complaint-driven data debugging system specialized for workflows integrating ML inference into SQL queries.  Our proposed approach combines tools from influence analysis and database provenance to solve the problem holistically.  Experimental results show that specifying complaints over query outputs can be as effective at detecting training data corruptions as manually correcting hundreds of model mispredictions.  \n","tags":["Data Cleaning","Data Debugging"],"title":"Towards Complaint-driven ML Workflow Debugging","type":"publication"},{"authors":["","Lampros Flokas","Eugene Wu","Jiannan Wang"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"9cb9e206ab8d230c625024c6172429a1","permalink":"https://wooya.me/publication/sigmod-2020-rain/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/sigmod-2020-rain/","section":"publication","summary":"As the need for machine learning (ML) increases rapidly across all industry sectors, there is a significant interest among commercial database providers to support \"Query 2.0\", which integrates model inference into SQL queries. Debugging Query 2.0 is very challenging since an unexpected query result may be caused by the bugs in training data (e.g., wrong labels, corrupted features). In response, we propose Rain, a complaint-driven training data debugging system. Rain allows users to specify complaints over the query's intermediate or final output, and aims to return a minimum set of training examples so that if they were removed, the complaints would be resolved. To the best of our knowledge, we are the first to study this problem. A naive solution requires retraining an exponential number of ML models. We propose two novel heuristic approaches based on influence functions which both require linear retraining steps. We provide an in-depth analytical and empirical analysis of the two approaches and conduct extensive experiments to evaluate their effectiveness using four real-world datasets. Results show that Rain achieves the highest recall@k among all the baselines while still returns results interactively.  \n","tags":["Data Cleaning","Data Debugging"],"title":"Complaint-driven Training Data Debugging for Query 2.0","type":"publication"},{"authors":null,"categories":["post"],"content":"If you are a fan of MLO or Omnifocus, I bet you will really miss the nextaction feature if you migrate to Todoist. With nextaction, one can keep really focus on things he should do, which is really nice. But since Todoist hasn’t provided this feature(maybe they don’t want this forever), we need to exploit the todoist api to achieve our goal.\nInspired by here, implementing by a polling server is a trivial but decent way. Rather than using python, I chose to write it in Rust. The rationale behind is that it should be a simple application with as less as possible dependencies, not python with lots of redundant libraries. And you can see after that the compiled docker image is only 18M which is much easier to distribute (Actually I’m considering a “nextaction as a service”, a.k.a. NaaS).\nUsage Simple run You should set environment variable NXTT_token to your Todoist token.\nTo build the application, rust is needed. Run: git clone \u0026amp;\u0026amp; cargo run --release\nDocker Image A docker image is also available. Run it by docker run -it -e NXTT_token=\u0026lt;your Todoist token\u0026gt; wooya/nextaction\nIntroduction  copy-pasted from the README\n @nextaction Nextaction will auto tag current nextaction task with @nextaction. It also supports parallel task (with ‘-’ append) and sequential task (with ‘:’ append).\ne.g.\n|-taskA: |-taskB // This task will be tagged @nextaction |-taskC: |-taskD  after you complete taskB, it will become\n|-taskA: |-taskC: |-taskD // This task will be tagged @nextaction  And for parallel tasks:\n|-taskA- |-taskB // This task will be tagged @nextaction |-taskC // This task will also be tagged @nextaction  So that you can add a filter on @nextaction to make you focused.\nParallel tasks and sequential tasks can corporate with each other seamlessly:\n|-taskA- |-taskB: | |-taskC // This task will be tagged @nextaction | |-taskD |-taskE // This task will be tagged @nextaction  @someday Nextaction also supports a tag called @someday. The logic is: when Nextaction meets a task which should be tagged @nextaction but currently has tag @someday, it won’t tag @nextaction to that task. So that your someday tasks won’t show up on your nextaction list.\nSo, don’t hesitate to deploy a nextaction server and let’s call it a day!\n","date":1491471688,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"5fb16b32cb4d19d002230f0c8919a08f","permalink":"https://wooya.me/post/todoist-nextaction/","publishdate":"2017-04-06T17:41:28+08:00","relpermalink":"/post/todoist-nextaction/","section":"post","summary":"Using `nextaction-rs`, you can now mimic Omnifocus or MLO's nextaction behavior in Todoist!","tags":["Todoist","GTD","nextaction","OmniFocus"],"title":"Implementing Nextaction In Todoist","type":"post"},{"authors":null,"categories":["project"],"content":"","date":1491467412,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"9136f8c1d00f4b197a4a96f5f26800e6","permalink":"https://wooya.me/project/nextaction-rs/","publishdate":"2017-04-06T16:30:12+08:00","relpermalink":"/project/nextaction-rs/","section":"project","summary":"","tags":["open source"],"title":"nextaction-rs","type":"project"},{"authors":null,"categories":["project"],"content":"","date":1490757454,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"fed11bd06a289fd96d116ddc359154f8","permalink":"https://wooya.me/project/tldextract-rs/","publishdate":"2017-03-29T11:17:34+08:00","relpermalink":"/project/tldextract-rs/","section":"project","summary":"","tags":["open source"],"title":"tldextract-rs","type":"project"},{"authors":null,"categories":["project"],"content":"It tailors pdf into kindle readable size in a foolproof way, details.\n","date":1490757232,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"3e963f4636128beb41c33baa5a9ce180","permalink":"https://wooya.me/project/k2pdfopt-service/","publishdate":"2017-03-29T11:13:52+08:00","relpermalink":"/project/k2pdfopt-service/","section":"project","summary":"It tailors pdf into kindle readable size in a foolproof way, details.","tags":["public service"],"title":"k2pdfopt service","type":"project"},{"authors":null,"categories":["project"],"content":"A sidekiq job format compatible server written in rust. So now you have a worker server running jobs 24/7 !\n","date":1490755639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"918eb0b9a63c59b790abc0a26fb6b8e7","permalink":"https://wooya.me/project/sidekiq-rs/","publishdate":"2017-03-29T10:47:19+08:00","relpermalink":"/project/sidekiq-rs/","section":"project","summary":"A sidekiq job format compatible server written in rust. So now you have a worker server running jobs 24/7 !","tags":["open source"],"title":"sidekiq-rs","type":"project"},{"authors":null,"categories":["talk"],"content":"","date":1490421600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"c23521694578c501ebb3f23b878da9b3","permalink":"https://wooya.me/talk/using-nif-to-handle-both-compute-bound-io-bound-tasks-in-elixir-copy/","publishdate":"2017-03-25T14:00:00+08:00","relpermalink":"/talk/using-nif-to-handle-both-compute-bound-io-bound-tasks-in-elixir-copy/","section":"talk","summary":"A real application in Strikingly which uses Rust and Elixir to handle both IO bounded and computing bounded task in single app","tags":["elixir","rust","io"],"title":"Using NIF to handle both compute-bound \u0026 IO-bound tasks in elixir","type":"talk"},{"authors":null,"categories":["post"],"content":"Update: Due to lack of funding, I decided to move it to another service provider which brings in new restrictions to this service:\n Total file size for a single email may not be larger than 6 MB. So if you have multiple files to convert, please send them in separate emails Conversion of Each file should be finished within –5– 15 mins, which means your file cannot have many pages. So if you want to convert a whole book, please cut them into small batches. In principle, this service suits most for paper reading.  Update, take 2: I integrated a small hack into the service as the first feature in 2018, so there’s no restriction on file size anymore. Update, take 3: I increased the time budget for each conversion to 15 mins and 1G memory.\nWhat it is? I’m fond of reading academic things on my Kindle, however, most academic papers are not designed to be read on a 6-inch size screen, let alone those with two columns format.\nI took some time writing a service and now things go easy. You can just send your pdf to kindle-deliver@wooya.me with your Send-To-Kindle address as the subject, then a cropped version of your book will be pushed to your Kindle automatically.\nBasically, this service will crop your book into a Kindle readable format and send it pretending it is you sending the book by mocking the sender address.\nSo, notice that the email used to send PDFs to kindle-deliver@wooya.me should be in your Approved Personal Document E-mail List.\nPage selection if file too large One thing is that if the book contains too many pages, the mailer may reject sending the attachment for its huge size, but you can enter a subset of pages (e.g. 1-9,13,209-) in the email content to shrink the result file size. The input format is:\n Single Page - e.g. 1 Some Pages - e.g. 5-9 Page Since - e.g. 10-  And use comma , to combine the page selections, e.g. 1,5-9,10-, with a prepending #  to indicate it’s a page selection command. There should not be any spaces inside the page selection.\nAdditional params Currently, this service supports params including page selection and raw k2pdfopt params by writing them in mail body. Only one PDF file should be attached if params are used.\nDetailed grammar:\n Prepending # with a space to do page selection, e.g. # 1,5-9,10- (from the above example). Prepending @ with a space to do pass raw params to k2pdfopt, e.g. @ -p 1,5-9,10- is same as # 1,5-9,10- if you know the param -p is page selection for k2pdfopt, resulting in k2pdfopt -p 1,5-9,10- -o \u0026lt;filename\u0026gt;_converted.pdf \u0026lt;filename\u0026gt; be called (if you know how to use k2pdfopt, then you will definetely understand this. Otherwise, #  is enough for you).  Caveat Be careful, you should not send classified documents to this address! Although I will delete them in place on the server, the document will exist in Gmail trash for several days until Google purge the trash bin automatically. And if something goes wrong, I may replay attachments in the trash bin to debug. So please DON’T SEND CLASSIFIED or PRIVATE DOCUMENTS to me in order to keep me away from legal issues.\nThanks Great thanks to willus who provided this excellent tool to produce the kindle readable version of pdf.\n   Do you like the service?  It costs me several bucks to maintain the server and email sending service  I\u0026#39;m really appreciate it if you can offer me a cup of coffee :)\n -- OR -- Bitcoin  1DyAXYcQy4HNWu83vRho1b8b15zqKn9qd6  ","date":1482399379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"0788365582e51f2a9159c55fdde67e6b","permalink":"https://wooya.me/post/kindle-pdf-optimization-service/","publishdate":"2016-12-22T17:36:19+08:00","relpermalink":"/post/kindle-pdf-optimization-service/","section":"post","summary":"By sending your pdf documents to `kindle-deliver@wooya.me` with your Send-To-Kindle address as subject, you will receive a cropped version of pdf in your Kindle which has a suitable page size for reading.\n","tags":["kindle","public service"],"title":"Kindle PDF Optimization Service","type":"post"},{"authors":null,"categories":["post","python"],"content":"本文讲述了笔者的multipledispatch2库的一些技术细节.\n由于是从静态语言切换成的python使用者, 笔者对python的动态类型非常不适应. 因此几乎每个地方都会使用multipledispatch给保护一下.\n然后用久了感觉有几个不方便:\n multipledispatch不支持函数参数的类型签名. python 3.5加入了typing库, 表示def foo(bar: int, baz: str) -\u0026gt; list这种写法是官方提倡的, 然而multipledispatch却不支持这种写法. multipledispatch不支持一个类型是多个类型的子类型的写法, 比如我想表达A \u0026lt;: B \u0026amp;\u0026amp; A \u0026lt;: C时就无能为力了.  由于给作者提issue以后作者几个月没动静, 因此在multipledispatch的基础上, 笔者修改了一些代码, 发布了multipledispatch2.\n主要改动是:\n  增加了对type annotation的支持, 即: 对于原来的\n@dispatch(int, str) def foo(a, b): pass  现在可以写作:\n@dispatch def foo(a: int, b: str): pass  相对更自然了一点.\n  增加了对subtype of multiple types的支持:\nclass A: pass class B: pass class C(A, B): pass @dispatch def foo(a: [A, B]): pass  对于调用foo(x), 当且仅当入参x同时是A和B的子类时, 对foo的调用才会成立. 在上例中只有foo(C())是成立的.\n这个新特性对于写库的人来说十分方便, 比如当你想要使用trait来做mixin的时候, users可能会拿你的trait混合出很多subtypes. 如果你想要提供一些函数来操作这些subtypes, 比如有一个函数当参数mixin了class A时产生behavior1, mixin了class B时产生behavior2, 同时mixin了class A和class B时, 产生behavior3. 在这种情况下, 使用原有的multipledispatch是不可能的.\n  技术细节: 主要问题在于dispatch order上. 比如: 对于同一个函数名foo, 它有两种类型:\n foo(a: A, b: B): pass foo(a: C, b: B): pass  其中:\nclass A: pass class B: pass class C(A): pass class D(C): pass  当你传入参数(C, B)的时候, 显然希望调用的是2而不是1, 当传入(D, B)的时候, 显然也希望调用2而不是1.只有当传入(A, B)时, 才希望调用的是1.\n用形象的话来说: 对于入参X, 希望被调用的函数是拥有最具体的签名的那个函数.\n于是这就要求我们有一个排序方法将foo的所有签名进行排序, 越具体的签名尽可能在前面. 进行签名搜索时, 应该从头开始搜, 并采纳第一个适合的签名.\n还是上面那个例子, 如果我们能够产生一个搜索顺序: [(C, B), (A, B)], 那不就符合要求了?\n对于排序, 显然需要一个操作符compare进行比较. 那么对于签名的compare该如何定义呢? 即如何确定签名与签名之间谁大谁小.\n在这里我们定义compare如下:\n令 A, B 为 tuple of types if A.length == b.length then A compare B := A \u0026lt;: B (即A是B的子类型) else return not comparable  而对于tuple of types的\u0026lt;:定义如下:\nfor zip(all a in A, all b in B): a \u0026lt;: b  即A中每一个类型都是对应位置上B的子类型时, A \u0026lt;: B\n这样我们定义了类型签名之间的subtyping关系. 于是我们使用这个compare关系对一个函数的所有签名进行拓扑排序, 结果将得到一个序列.\n注意这里使用拓扑排序而不是别的排序方法, 是因为签名之间并不是良序关系. 两个签名之间可能其实是没有任何大小关系的(上面的return not comparable分支). 因此一个函数的所有签名其实构成了多个DAG(有向无环图).\n按照拓扑排序得到的序列搜索, 一定能够得到最具体的那个签名.\n加入multiple subtypes后的变化: 上面的定义很不错, 但是没有考虑一种情况: 在tuple of types的\u0026lt;:定义中, 我们使用了a \u0026lt;: b这个比较. 然而当multiple subtypes存在的时候, a和b可能是一个类型, 也可能是一个联合类型[type, type ...]. 那么如何求a \u0026lt;: b呢?\n比如, 当\nclass A: pass class B: pass class C(A, B): pass  时, 如何证明C \u0026lt;: [A, B]呢?\n甚至是当\nclass A: pass class B: pass class C(A): pass class D(B): pass  时, 如何证明[C, D] \u0026lt;: [A, B]呢?\n我们拓展一下subtyping关系即可:\n对于types a and b def a \u0026lt;: b as if a and b both are type then return a \u0026lt;: b else if a is type and b is tuple then return if a \u0026lt;: all types in b else if a is tuple and b is type then return if any types in a \u0026lt;: b else if a and b both are tuple then return for all types in b if any types in a \u0026lt;: types in b  经过这样一个补充定义以后, multipledispatch2就完美支持multiple subtypes啦.\n","date":1463136088,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"cf4f775af27caad9938b811e3878386b","permalink":"https://wooya.me/post/multipledispatch2/","publishdate":"2016-05-13T18:41:28+08:00","relpermalink":"/post/multipledispatch2/","section":"post","summary":"The article elaborates some technic details in building a multiple dispatch library in python. \n","tags":["subtyping","type system","multiple dispatch"],"title":"Multipledispatch2","type":"post"},{"authors":null,"categories":["post","python"],"content":"本文讲解了python的方法搜索优先级, super函数的调用以及python的线性化算法C3.\n昨天在搞python多继承的时候遇到一个问题抽象化如下:\nclass A: def foo(self): print(\u0026#34;A\u0026#34;) class B: def foo(self): print(\u0026#34;B\u0026#34;) super().foo() class C(B,A): def foo(self): print(\u0026#34;C\u0026#34;) super().foo()  调用C().foo()会输出:\nC B A  其中第一行输出C很好理解, 第二行类B的foo输出B以后调用了super的foo方法。但是如果B的foo:\n 调用了B的super, 则因为B并没有父类, 所以super().foo()的调用应该会失败，抛出AttributeError错误。 调用了self的super, 由于此时self的类型是C, 那么又会去调用B的foo形成无限递归调用.  然而事实上并没有发生以上两种情况。B的super竟然调用了A的foo.\n其实个人觉得这里的super有些误导人, super并不一定是去寻找父类, 它的意思是: 沿着方法搜索序列(mro)往上搜索一格。 要理解mro, 首先需要知道什么是linearization.\nlinearization一般出现在具有多继承的语言中, 比如scala, python等. 因为多继承必然会带来方法冲突等问题从而导致方法搜索失败, 所以必须规定一个方法搜索顺序防止冲突, 线性的从最底部叶 子类开始向上搜索方法直到找到或失败. 这就要求把一棵继承树变化成一个一维的线性结构.\n在python中线性化的算法是一种叫做C3的算法. 来自论文A Monotonic Superclass Linearization for Dylan.\n它的描述如下(来自wikipedia):\n 对于以下的类型:\n  class O class A extends O class B extends O class C extends O class D extends O class E extends O class K1 extends A, B, C class K2 extends D, B, E class K3 extends D, A class Z extends K1, K2, K3\n\u0026gt; 他们的线性化(即方法搜索顺序)是: \u0026gt; ``` L(O) := [O] // the linearization of O is trivially the singleton list [O], because O has no parents L(A) := [A] + merge(L(O), [O]) // the linearization of A is A plus the merge of its parents\u0026#39; linearizations with the list of parents... = [A] + merge([O], [O]) = [A, O] // ...which simply prepends A to its single parent\u0026#39;s linearization L(B) := [B, O] // linearizations of B, C, D and E are computed similar to that of A L(C) := [C, O] L(D) := [D, O] L(E) := [E, O] L(K1) := [K1] + merge(L(A), L(B), L(C), [A, B, C]) // first, find the linearizations of K1\u0026#39;s parents, L(A), L(B), and L(C), and merge them with the parent list [A, B, C] = [K1] + merge([A, O], [B, O], [C, O], [A, B, C]) // class A is a good candidate for the first merge step, because it only appears as the head of the first and last lists = [K1, A] + merge([O], [B, O], [C, O], [B, C]) // class O is not a good candidate for the next merge step, because it also appears in the tails of list 2 and 3, but... = [K1, A, B] + merge([O], [O], [C, O], [C]) // ...class B qualified, and so does class C; class O still appears in the tail of list 3 = [K1, A, B, C] + merge([O], [O], [O]) // finally, class O is a valid candidate, which also exhausts all remaining lists = [K1, A, B, C, O] L(K2) := [K2] + merge(L(D), L(B), L(E), [D, B, E]) = [K2] + merge([D, O], [B, O], [E, O], [D, B, E]) // select D = [K2, D] + merge([O], [B, O], [E, O], [B, E]) // fail O, select B = [K2, D, B] + merge([O], [O], [E, O], [E]) // fail O, select E = [K2, D, B, E] + merge([O], [O], [O]) // select O = [K2, D, B, E, O] L(K3) := [K3] + merge(L(D), L(A), [D, A]) = [K3] + merge([D, O], [A, O], [D, A]) // select D = [K3, D] + merge([O], [A, O], [A]) // fail O, select A = [K3, D, A] + merge([O], [O]) // select O = [K3, D, A, O] L(Z) := [Z] + merge(L(K1), L(K2), L(K3), [K1, K2, K3]) = [Z] + merge([K1, A, B, C, O], [K2, D, B, E, O], [K3, D, A, O], [K1, K2, K3]) // select K1 = [Z, K1] + merge([A, B, C, O], [K2, D, B, E, O], [K3, D, A, O], [K2, K3]) // fail A, select K2 = [Z, K1, K2] + merge([A, B, C, O], [D, B, E, O], [K3, D, A, O], [K3]) // fail A, fail D, select K3 = [Z, K1, K2, K3] + merge([A, B, C, O], [D, B, E, O], [D, A, O]) // fail A, select D = [Z, K1, K2, K3, D] + merge([A, B, C, O], [B, E, O], [A, O]) // select A = [Z, K1, K2, K3, D, A] + merge([B, C, O], [B, E, O], [O]) // select B = [Z, K1, K2, K3, D, A, B] + merge([C, O], [E, O], [O]) // select C = [Z, K1, K2, K3, D, A, B, C] + merge([O], [E, O], [O]) // fail O, select E = [Z, K1, K2, K3, D, A, B, C, E] + merge([O], [O], [O]) // select O = [Z, K1, K2, K3, D, A, B, C, E, O] // done  比如要调用Z().foo(), 然而D和A都定义了foo这个方法, 则根据Z的线性化L(Z) := [Z, K1, K2, K3, D, A, B, C, E, O], 第一个搜索到的foo应该来自D。\n这样的话就完美解释了上文第一个例子中为什么B的super().foo()调用了A的foo:\nsuper()其实是super(__class__, self)的简写, 它的作用是在self的线性化上排除掉自己以及自己之前的类型.\n比如B中的super()其实是super(__class__, self)就是super(B, self). 其中self为C.\n比如因为C线性化为L(C) := [C, B, A], C中调用super, 在线性化结果上排除自己以及之前的类型，则产生搜索顺序[B, A], 所以C中super().foo()调用的结果是调用了B的foo.\n而在B中调用super()排除自己B以及之前的类型C, 产生搜索顺序[A], 所以B中super().foo()调用的结果是调用了A的foo.\n这个线性化结果可以通过python的类的mro方法进行查看。\nC.mro() == [C, B, A] ","date":1460194501,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"766894def875f8cbf6d8eb77528f0d81","permalink":"https://wooya.me/post/python-c3-linearization/","publishdate":"2016-04-09T17:35:01+08:00","relpermalink":"/post/python-c3-linearization/","section":"post","summary":"This article describes the linearization algorithm used in python's `super` function call.\n","tags":["linearization"],"title":"Python C3 Linearization","type":"post"},{"authors":null,"categories":["project"],"content":"Coroutine library for Rust! As the cooperator of this project with Zonyitoo.\n","date":1446609791,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"a4ddd4542abf4eae78626f7363f6beb6","permalink":"https://wooya.me/project/coroutine-rs/","publishdate":"2015-11-04T12:03:11+08:00","relpermalink":"/project/coroutine-rs/","section":"project","summary":"Coroutine library for Rust! As the cooperator of this project with Zonyitoo.","tags":["coroutine","async","open source"],"title":"coroutine-rs","type":"project"},{"authors":null,"categories":["post","rust"],"content":"rust 提供了一个很强大的编译时功能：自定义编译器插件。\n通过向编译器register一个函数作为入口，它可以在lint时期把ast作为register的那个函数的一个参数来invoke。也就是说，通过编译器插件，我们可以做很多强（wei）大（suo）的事情。\n 例子1：rust-chamber  chamber是一个语言级别的sandbox(其实是一个rustc的包装)，可以防止你的语言中出现不安全的code。目前它的功能很简单，一旦发现你的code里面有使用了unsafe，或者开启了编译器feature(#![feature()]), 或者使用了不安全的crate（比如intrinsic）编译就会通不过。怎么实现的呢？其实很简单。下面贴上核心代码：\n fn check_expr(\u0026amp;mut self, ctx: \u0026amp;Context, e: \u0026amp;ast::Expr) { match e.node { // Don\u0026#39;t warn about generated blocks, that\u0026#39;ll just pollute the output. ast::ExprBlock(ref blk) if blk.rules == ast::UnsafeBlock(ast::UserProvided) =\u0026gt; { ctx.span_lint(CH_UNSAFE_BLOCK, e.span, \u0026#34;chamber: `unsafe` block\u0026#34;); } _ =\u0026gt; () } }  context是用来控制编译器行为的。e是传入的expression的ast表现形式。check_expr的作用就是对每个传入的expression进行判断，如果是unsafe block，就报错。\n 例子2：spellck  这是一个基于字典的拼写检查插件。通过字典，可以将代码中的单词拼写错误在编译器进行提示。麻麻再也不用担心我在代码里面打错别字啦！\n 例子3：regex  重量级插件：rust官方正则表达式库。什么？正则表达式和插件有什么关系？ 这里说的是rust的regex！宏。众所周知，正则表达式需要进行编译。比如python的re.compile，go的regexp.Compile等等。正则表达式需要在运行时，将一个正则表达式字符串编译成正则表达式虚拟机上的指令。所以正则表达式其实是一个内嵌在语言内部的虚拟机语言。 既然正则表达式在运行时才能够进行编译，并且只是编译成虚拟机指令，那么它的效率必然会比原生代码低很多。那么我们能不能把这个过程提前到编译时呢？正则表达式规则是通过字符串的形式来书写的，在编译时无法确定它的内容（字符串是可变的）。所以在一般的语言中确实做不到编译时处理正则表达式。但是通过rust的编译器插件，我们可以实现对常量字符串表达式进行编译时的正则表达式编译。 regex插件在编译时会把相应的正则表达式编译成rust代码，所以在运行时完全没有用到正则的虚拟机和指令。因此执行速度非常快。但唯一的缺点就是如果生成太多的正则表达式，那么编译出来的二进制文件会变得非常大。（毕竟是吧正则表达式展开成了大量的rust代码。）\n================================================================\n介绍到此为止，下面是教程。 首先，你需要引入一个编译器特性， plugin_registrar 在代码开头加上 #![feature(phase, plugin_registrar)] 使用phase特性是因为我们需要一些在rustc里面的宏来帮忙。可以省去一些代码。\n#[phase(plugin, link)] // Load rustc as a plugin to get lint macros extern crate rustc; extern crate syntax; use rustc::lint::{Context, LintPass, LintArray}; use rustc::plugin::Registry; use syntax::ast;  引入一些crate。 phase是一个编译器特性。其中plugin的意思是把crate当作插件插入到当前代码（为了引入其中的macro。因为macro不能像传统变量那样采用use来引入），link的意思是把该crate连接到此文件（crate默认其实有一个link的feature）。 接下来是建立一个lint的属性：\npub static mylint: \u0026amp;\u0026#39;static rustc::lint::Lint = \u0026amp;lint_initializer!(mylint, Deny, \u0026#34;abrakadabra\u0026#34;);  我们把它起名字叫做mylint。默认级别是Deny, 并且把它绑定到了mylint这个static变量上（需要注意的是，Lint变量必须是static的）。描述随便写了点。 其中，lint的级别有四个，分别是Allow,Warn,Deny,Forbid.其中Forbid和Deny的区别在于，如果是Deny，那么在源代码里面，使用者可以通过#[allow(myliny)],#[warn(mylint)]进行lint级别更换。比如从禁止一个特性转变为只是警告（P.S. 那就没用啦！程序员从来不看警告）。但是如果设置为Forbid级别，那么用户就无论如何也没办法改变你的lint级别了。比如上面的例子，chamber里面，作者就使用了Forbid级别。 然后我们新建一个LintPassObject;\nstruct MyLintPass1;  要成为一个LintPassObject，还需要一个Trait的帮助。\nimpl LintPass for MyLintPass1 { fn get_lints(\u0026amp;self) -\u0026gt; LintArray { lint_array!(mylint) } }  我们需要为我们的LintPassObject实现LintPass这个Trait。 LintPass这个Trait有很多方法，但是我们只要实现get_lint这一个就可以了。get_lint这个方法的含义就是把我们前面建立的那个mylint这个lint和我们的LintPassObject关联起来。lint_array这个宏的作用就是生成一个static的lint array。 然后，最后一件事，就是把我们的LintPassObject注册进编译器。\n#[plugin_registrar] pub fn plugin_registrar(reg: \u0026amp;mut Registry) { reg.register_lint_pass(box MyLintPass1); }  只要在一个函数上面打上\n#[plugin_registrar]  就告诉了编译器，下面那个函数，是要cha进你身体里的！ 然后我们就完成了一个最简单的编译器插件。把它编译成dylib（注意必须是动态链接库，不能编译成rlib） 在需要使用它的地方写上\n#![feature(phase)] #[phase(plugin)] extern crate plugin;  就ok啦！ 于是我们完成了一个什么都不做的编译器插件。\n接下来我们给我们的插件添加点小功能：阻止编译啊哈哈哈！ 在这个地方，我们加一个函数实现\nimpl LintPass for MyLintPass1 { fn get_lints(\u0026amp;self) -\u0026gt; LintArray { lint_array!(mylint) } fn check_crate(\u0026amp;mut self, ctx: \u0026amp;Context, crt: \u0026amp;ast::Crate) { ctx.lint(mylint, \u0026#34;deliberate fail!\u0026#34;) } }  于是除非你在main函数打标记#[allow(mylint)]否则编译就是通不过啦啦啦\n","date":1446609256,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"6bc076099f3e39eaf1bb899cf4acf3f6","permalink":"https://wooya.me/post/rust-compiler-plugin/","publishdate":"2015-11-04T11:54:16+08:00","relpermalink":"/post/rust-compiler-plugin/","section":"post","summary":"We can use compiler plugin in rust compiler pipeline in order to enhance the grammar.\n","tags":["compiler"],"title":"Rust Compiler Plugin","type":"post"},{"authors":null,"categories":["post","rust"],"content":"rust和c/c++一样,可以内联汇编.语法和c/c++的内联汇编大致一样.只有几个细节稍有不同.\n首先,需要开启一个特性 #![feature(asm)]\n然后在asm!宏里面写汇编即可.\n格式是:\nasm!(assembly template : output operands : input operands : clobbers : options );  大致和c/c++相同.其中有几个不同点:\n在最后一段用来声明已经使用过的寄存器的那一段(clobbers)下面还可以跟一段option段.备选项有:“intel\u0026#34;表示采用intel汇编而不像c那样用AT\u0026amp;T汇编.“volatile”,和c里面的__asm__volatile__一样.“alignstack”,让编译器自动插入对齐栈的代码(因为有些指令集需要对齐栈,比如SSE指令集). 在填充模版的时候,变量用$0,$1来表示,而不是c的%1,%2来表示. 立即数用$$表示,$$1就是1. 寄存器直接用%来表示,%eax表示eax寄存器 模版里面多条指令用分号(;)来分割,而不是c的”\\r\\n\u0026#34; 声明clobber的时候直接写eax,不用像C那样写%eax 记得asm!的时候要外面套unsafe块 intel语法我没有试过.所以无从比较语法区别:(\n下面附上我的一个小例子,用汇编+偏移量来访问数组.\n#![feature(asm)] fn main() {asm();} #[cfg(target_arch = \u0026#34;x86_64\u0026#34;)] fn asm() { use std::mem::transmute; use std::rand::random; let array: \u0026amp;[u64] = \u0026amp;[random(),random(),random(),random()]; let address = unsafe { transmute::\u0026lt;_, (i64, i64)\u0026gt;(array).0 }; for offset in 0u64..4 { let ret: u64; unsafe { asm!( r\u0026#34; mov ($1, $2, 8), %rax; mov %rax, $0; \u0026#34; : \u0026#34;=r\u0026#34;(ret) : \u0026#34;r\u0026#34;(address), \u0026#34;r\u0026#34;(offset) : \u0026#34;rax\u0026#34; : ); } println!(\u0026#34;在第{}号位上的元素是{}\u0026#34;, offset, ret); } }  ","date":1446609210,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"497cfea55359f46f38c344f8b7242535","permalink":"https://wooya.me/post/rust-inline-asm/","publishdate":"2015-11-04T11:53:30+08:00","relpermalink":"/post/rust-inline-asm/","section":"post","summary":"A simple introduction about using asm in rust.","tags":["asm"],"title":"Rust Inline ASM","type":"post"},{"authors":null,"categories":["post","rust"],"content":"本文通过解析llvm的ir来解析rust enum的内存布局。\nrust的enum差不多是C的enum和union类型的混合体。可以写成\nenum Enum { AEnum = 0, BEnum = 1, CEnum = 2, } // 类似C的Enum  也可以\nenum Buk { two_int(int,int), three_uint(uint,uint,uint), lonely_f64(f64), } // 类似union  甚至\nenum LinkedListNode\u0026lt;T\u0026gt; { DataOnly(T), DataWithNext(T,Box\u0026lt;LinkedListNode\u0026lt;T\u0026gt;\u0026gt;), }  总之，利用rust的enum，可以任意的构造想要的抽象数据结构。 在另一些语言中，这种数据结构叫做ADT(algebra data type, 代数数据类型)。\n===============================================\n 最普通的enum。  有如下结构：\nenum simple { A,B,C,D }  在rust编译完以后，完全不会为simple生成任何的数据结构。 let a = A 只会被rust编译成 let a: i8 = 0。同理，B就是1i8，C就是2i8.\n同理\nenum simple { A = 1, B = 2, C = 3, D = 4, }  也只是把let a = 0i8变成了let a = 1i8而已。\nmatch这样一个最简单的enum，rust所做的仅仅只有一个简单的对i8的switch语句。\n union like enum  考虑如下enum：\nenum simple { A(i32,i64), B(u8), C(f32), }  这时，这个enum的llvm内存表示为{ i8, [7 x i8], [1 x i64] }，用rust来表示就是(i8,i8,i8,i8,i8,i8,i8,i8,i64)\n一共8个i8，一个i64. 可能会纳闷：怎么会有那么多i8呢？ 原因在于：内存对齐。\n首先，enum的标号肯定位于头部，就是第一个i8.因此A还是0，B还是1. 但是和例1不同的是，本例中的enum是带有自定义数据的。所以必须为自定义数据分配空间。 我们来算一下：一共需要1i8（用于标号）+1i32（A的第一个域）+164（A的第二个域）。B和C我们就不管了，因为它们的大小都小于A，因此我们可以复用A的内存空间。所以这样一个enum至少需要104个bit。但是由于内存对齐的原因，第一个i8要和后面的i64对齐，所以要补上7个i8，因为78+8=64嘛。 补完之后我们发现，那个i32也可以被用来补完的7个i8来表示了（4个i8凑一凑就一个i32了嘛）。于是这个enum就是8个i8+1个i64组成了。\n现在来看看当它们分别是A，B，C时的情况。 当enum是A的时候，它的结构会转换成{i8, i32, i64}.第一个用于标号，是0.第二个是A的第一个域，i32，i64是第二个域。用于内存对齐的i8被llvm自动藏起来了。完整的表示是{i8, [3 x i8], i32, i64}，中间3个i8会隐藏掉。 当enum是B的时候，结构就变成了{i8,i8}后面的一大堆东西统统不要。所以如果一个enum要是被这样构造了，每一个B都会浪费大量的空间（空间使用率16/128=12.5%）。对于单片机编程来说要尽量避免这种情况。 当enum是C的时候，结构是{i8,float},[3xi8]同A，会隐藏起来。后面浪费了一个i64.利用率50%，也挺低。\n 1和2混搭  如下enum：\nenum simple { A(i32,i64), B(u8), C(f32), D, E }  具体情况和union enum差不多。后面两个D和E变成{i8}罢了。\n 泛型enum  enum simple\u0026lt;T\u0026gt; { Data(T), Nil } let a = simple::Data::\u0026lt;int\u0026gt;(1); let b = simple::Nil::\u0026lt;uint\u0026gt;;  泛型其实只是生成了两份代码，一个叫%\u0026#34;enum.main::simple\u0026lt;[int]\u0026gt;\u0026#34; = type { i8, [7 x i8], [1 x i64] }， 另一个叫%\u0026#34;enum.main::simple\u0026lt;[uint]\u0026gt;\u0026#34; = type { i8, [7 x i8], [1 x i64] }，其它无任何差别。\n struct variant  enum simple { duck {a: int, b: int}, Nil } let a = simple::duck{a:4,b:3};  和普通的enum一样，只是给duck里面的两个i64起个名字罢了。在ir层面没有任何区别。\n #[repr(C)]  rust还支持把enum的内存表示变为C风格的。\n#[repr(C)] enum simple { a(int), b,c }  这儿的区别在于，头部的标号和内存对齐，全部从i8变成了i32，即C的int。{i32, [1 x i32], i64}\n","date":1446609099,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"1a1383632f7a92dc16caac27f234186e","permalink":"https://wooya.me/post/rust-enum/","publishdate":"2015-11-04T11:51:39+08:00","relpermalink":"/post/rust-enum/","section":"post","summary":"We analyzed the memory structure of rust in this article through the perspective of llvm ir.\n","tags":["enum","memory detail"],"title":"Rust Enum","type":"post"},{"authors":null,"categories":["post","rust"],"content":"rust有非常强大的类型系统。今天我们来说说rust的泛型。\nrust有两种泛型：\n  基于static dispatch的泛型，类似于C++的模板。在编译期进行代码特化（monomorphization），为每一种类型生成一份代码。好处是执行效率高，但是会带来额外的冗余代码，使二进制文件变大（bloat）。\n  基于dynamic dispatch的泛型，类似于java和go的interface。在运行期查找虚表（vtable）来选择执行的方法。好处是使用灵活，但是性能肯定比static dispatch来的差。本篇着重介绍这一种泛型。\n    Trait Object  rust的dynamic dispatch实现都是基于一种叫做trait object的类型来实现的。先看一个例子：\ntrait Object { fn dood(\u0026amp;self) -\u0026gt; int { 1i } } impl Object for int {} impl Object for uint {} fn main() { fn gimme_an_object(i: \u0026amp;Object) { println!(\u0026#34;{}\u0026#34;, i.dood()); } gimme_an_object(\u0026amp;2i); // OUTPUT: 1 gimme_an_object(\u0026amp;3u); // OUTPUT: 1 }  gimme_an_object函数这里发生了什么？ 可以看到，gimme_an_object需要传入一个\u0026amp;Object类型的参数。就是说，gimme_an_object函数的参数i是一个实现了Object这个trait的引用类型。所以我们无论喂给它了一个\u0026amp;int或一个\u0026amp;uint，它都能完成调用。因为之前的两个impl已经为uint类型和int类型实现了Object这个trait。 在这一点上，rust的trait和go的interface很相似。我们只需要传入一个接口，函数就能完成工作，为不用管传入的参数到底是什么类型。 但是这里有一个细节需要注意：为什么要写\u0026amp;Object，写成fn gimme_an_object(i: Object)不行吗？ 答案是不行。有人可能很奇怪，为什么我在go里面直接写interface就没问起，rust里面却必须要加个引用呢？ 原因有两个：\n  rust有三种原生指针，\u0026amp;、Box和*。无论哪一种都可以作为trait object的indirection，因此要是用interface一统江湖，不再写\u0026amp;，必然导致灵活性下降。无论用哪一种作为trait\nobject的默认指针都有失偏颇。   trait object的编译器魔法。  在rust里，所有的指针都是一个字长。比如64位机器上，\u0026amp;1i的大小就是64个bit。 但是在trait object中，rust编译器会隐式的把指针转换为一个胖指针。\n// in core::raw::TraitObject struct TraitObject { data: *mut (), vtable: *mut (), }  也就是说，所有的TraitObject大小其实都是两个字长。第一个指向数据，第二个指向虚函数表。这点和go的interface其实是一模一样的。\n trait safety  对于trait object，rust还有一个限制：只有safe的trait才能被用作trait object。 什么叫safe的trait呢？ 因为有些trait会返回一个self类型，比如：\ntrait RetSelf { fn ret_self(\u0026amp;self) -\u0026gt; Self; }  如果impl给了int，那么ret_self方法的返回值就是一个int，要是impl给了f64，那么返回值就是一个f64.这就意味着代码诸如：\nfn unsafe_object(i: \u0026amp;RetSelf) { let c = i.ret_self(); }  是无法编译的，因为无从知道c的大小。因此在rust里面，只有不带有fn() -\u0026gt; Self类型的方法的trait才叫safe的trait，只有safe的trait才能被用作trait object。这也是为什么rust有很多trait xxxx， trait XXXXEXT。因为XXXX是safe的object，而trait XXXXEXT里面包含了带有返回Self的方法。如果把两者合并为同一个trait，意味着trait XXXX将不能再用于trait object。因此必须用两个trait来吧unsafe的方法隔离开。比如常用的Iterator trait就是如此。它从以前的一个trait变成了如今的interator和iteratorExt.\n","date":1446608965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"89808a74955cf1dbbcd9a4de3e271389","permalink":"https://wooya.me/post/rust-object/","publishdate":"2015-11-04T11:49:25+08:00","relpermalink":"/post/rust-object/","section":"post","summary":"This article simply introduced the mechanism of dynamic dispatch in rust.  \n","tags":["OO","generic"],"title":"Rust Object","type":"post"},{"authors":null,"categories":["post","rust"],"content":"在具有高级类型系统的语言里面，有一种类型标记的用法叫phantom type, 比如Haskell语言。这种用法有个很有意思用途：用来做编译时的类型检查，并且对于编译后的代码来说，完全没有任何的副作用。\n举个例子，在做几何运算的时候，我们会碰到运算时变量单位的问题。\n#[deriving(Show)] struct Length\u0026lt;Num\u0026gt;(Num); type Meter = f64; type Inch = f64; let a: Length\u0026lt;Meter\u0026gt; = Length(5.0 as Meter); let b: Length\u0026lt;Inch\u0026gt; = Length(7.0 as Inch); impl\u0026lt;Num\u0026gt; Add\u0026lt;Length\u0026lt;Num\u0026gt;,Length\u0026lt;Num\u0026gt;\u0026gt; for Length\u0026lt;Num\u0026gt; where Num: Add\u0026lt;Num,Num\u0026gt; { fn add(self, rhs: Length\u0026lt;Num\u0026gt;) -\u0026gt; Length\u0026lt;Num\u0026gt; { Length(self.0 + rhs.0) } } println!(\u0026#34;{}\u0026#34;, a + b);  在上面的代码中，我们有一个很明显的bug,单位为米的a竟然和单位为英寸的b进行了相加。要是在业务逻辑中出现这么一茬，肯定很难debug，因为从语法上来讲，完全没有问题啊！\n现在我们来引入一种类型系统的trick，叫做phantom type。\nmod unit { #[deriving(Show)] enum Meter {} #[deriving(Show)] enum Inch {} } #[deriving(Show)] struct Length\u0026lt;Unit,Num\u0026gt;(Num); let a: Length\u0026lt;unit::Meter,f64\u0026gt; = Length(5.0); let b: Length\u0026lt;unit::Inch,f64\u0026gt; = Length(7.0); impl\u0026lt;Unit,Num\u0026gt; Add\u0026lt;Length\u0026lt;Unit,Num\u0026gt;,Length\u0026lt;Unit,Num\u0026gt;\u0026gt; for Length\u0026lt;Unit,Num\u0026gt; where Num: Add\u0026lt;Num,Num\u0026gt; { fn add(self, rhs: Length\u0026lt;Unit,Num\u0026gt;) -\u0026gt; Length\u0026lt;Unit,Num\u0026gt; { Length(self.0 + rhs.0) } } println!(\u0026#34;{}\u0026#34;, a + b);  为了打印方便，加了一些#[deriving(Show)]。 我们给Length类型加上了一个Unit\t的类型标记。但是，Unit却没有出现在任何有值的地方，它仅仅是作为了一个类型标记存在着。所以，我想这也是为什么它叫phantom type的原因吧。\n编译上面那段的代码，编译器会给你报错：\nerror: mismatched types: expected `main::Length\u0026lt;main::unit::Meter, f64\u0026gt;`, found `main::Length\u0026lt;main::unit::Inch, f64\u0026gt;` (expected enum main::unit::Meter,found enum main::unit::Inch)  编译器说： a是Length\u0026lt;Meter,f64\u0026gt;, b是\u0026lt;Inch，f64\u0026gt;,它们类型不相容。\n仅仅通过增加一个额外的类型标记，我们就实现了让编译器自动给我们检查单位的方法，而且这种phantom type的小trick，不会带来任何的运行时负担。在编译完毕后，它们就被一并的擦除了。\n其实这是一种对现实中单位制的模拟。平时我们写的时候一般写 1m，2mm之类。其实当我们写下了1m这个数字的时候，其实着我们写下了两个东西，作为数字的1和作为单位的m。单独拿出来，对于数字1来讲，他没有任何的意义。可以随便对他加上其它的数字；对于m来说，它仅仅是个单位，同样也没有意义。但是1m结合起来，m就为1赋予了class信息，改变了它的运算规则。phantom type在此，就起到了这个量纲作用。\n除了用来作为数字的单位以外，还有一个比较好用的地方是作为用户输入安全性的标记（这和rust着重安全是遥相呼应啊）。 众所周知，web安全有很大一部分问题，是因为没有对用户输入做检查。一个安全的系统应该对于所有的用户输入采取不信任的态度，应该由代码对其进行检查。 但是在开发的时候，可能由于程序员水平问题，或者一时疏忽等原因，往往会忘记检查。这就为系统埋下了很大的隐患。 利用phantom type我们可以为所有的string打上tag，让编译器来替我们进行检查。\nenum Trusted {} enum UnTrusted {} struct UserInput\u0026lt;T\u0026gt;(String); let a: UserInput\u0026lt;Trusted\u0026gt;(\u0026#34;safe\u0026#34;.to_string()); let b: UserInput\u0026lt;UnTrusted\u0026gt;(\u0026#34;SQL Injectiong is here!!!\u0026#34;.to_string()); insert_into_database(a); insert_into_database(b); // 编译通不过  假定我们的insert_into_database接受一个UserInput\u0026lt;Trusted\u0026gt;作为输入。那么那个b，我们是无论如何也无法存入数据库的，必须要我们进行显式的检查，将其转换为UserInput,否则编译必定报错。这样系统的安全性就大大的加强了。\n","date":1446431783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643765003,"objectID":"f8d7ab8def63c52083ae80369317e475","permalink":"https://wooya.me/post/rust-phantom-type/","publishdate":"2015-11-02T10:36:23+08:00","relpermalink":"/post/rust-phantom-type/","section":"post","summary":"Using phantom type when programming can noticeably increase the robustness of the code  by transfering the logic checking into type checking.\n","tags":["phantom type","type system"],"title":"Talk About Phantom Type","type":"post"}]